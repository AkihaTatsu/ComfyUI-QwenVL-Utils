{
  "base_dir": "LLM/GGUF",
  "qwenVL_model": {
    "Qwen3-VL-4B-Instruct-GGUF": {
      "author": "Qwen",
      "repo_name": "Qwen3-VL-4B-Instruct-GGUF",
      "repo_id": "Qwen/Qwen3-VL-4B-Instruct-GGUF",
      "mmproj_file": "mmproj-Qwen3VL-4B-Instruct-F16.gguf",
      "model_files": [
        "Qwen3VL-4B-Instruct-Q4_K_M.gguf",
        "Qwen3VL-4B-Instruct-Q8_0.gguf",
        "Qwen3VL-4B-Instruct-F16.gguf"
      ],
      "defaults": {
        "context_length": 8192,
        "image_max_tokens": 4096,
        "n_batch": 512,
        "gpu_layers": -1,
        "top_k": 0,
        "pool_size": 4194304
      }
    },
    "Qwen3-VL-8B-Instruct-GGUF": {
      "author": "Qwen",
      "repo_name": "Qwen3-VL-8B-Instruct-GGUF",
      "repo_id": "Qwen/Qwen3-VL-8B-Instruct-GGUF",
      "mmproj_file": "mmproj-Qwen3VL-8B-Instruct-F16.gguf",
      "model_files": [
        "Qwen3VL-8B-Instruct-Q4_K_M.gguf",
        "Qwen3VL-8B-Instruct-Q8_0.gguf",
        "Qwen3VL-8B-Instruct-F16.gguf"
      ],
      "defaults": {
        "context_length": 8192,
        "image_max_tokens": 4096,
        "n_batch": 512,
        "gpu_layers": -1,
        "top_k": 0,
        "pool_size": 4194304
      }
    },
    "Qwen3-VL-4B-Thinking-GGUF": {
      "author": "Qwen",
      "repo_name": "Qwen3-VL-4B-Thinking-GGUF",
      "repo_id": "Qwen/Qwen3-VL-4B-Thinking-GGUF",
      "mmproj_file": "mmproj-Qwen3VL-4B-Thinking-F16.gguf",
      "model_files": [
        "Qwen3VL-4B-Thinking-Q4_K_M.gguf",
        "Qwen3VL-4B-Thinking-Q8_0.gguf",
        "Qwen3VL-4B-Thinking-F16.gguf"
      ],
      "defaults": {
        "context_length": 8192,
        "image_max_tokens": 4096,
        "n_batch": 512,
        "gpu_layers": -1,
        "top_k": 0,
        "pool_size": 4194304
      }
    },
    "Qwen3-VL-8B-Thinking-GGUF": {
      "author": "Qwen",
      "repo_name": "Qwen3-VL-8B-Thinking-GGUF",
      "repo_id": "Qwen/Qwen3-VL-8B-Thinking-GGUF",
      "mmproj_file": "mmproj-Qwen3VL-8B-Thinking-F16.gguf",
      "model_files": [
        "Qwen3VL-8B-Thinking-Q4_K_M.gguf",
        "Qwen3VL-8B-Thinking-Q8_0.gguf",
        "Qwen3VL-8B-Thinking-F16.gguf"
      ],
      "defaults": {
        "context_length": 8192,
        "image_max_tokens": 4096,
        "n_batch": 512,
        "gpu_layers": -1,
        "top_k": 0,
        "pool_size": 4194304
      }
    }
  }
}
