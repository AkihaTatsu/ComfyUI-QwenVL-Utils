{
  "model_name": "Select the vision-language model. HuggingFace models offer full-precision inference; [GGUF] models use quantized llama.cpp backend for lower VRAM. First run downloads weights automatically.",
  "quantization": "Precision vs VRAM trade-off (HF models only, ignored for GGUF). FP16 gives the best quality; 8-bit suits 8-16 GB GPUs; 4-bit fits 6 GB or lower but is slower.",
  "attention_mode": "Attention backend selection (HF models only, ignored for GGUF).\n• auto: Auto-select best available (recommended)\n• flash_attention_2: External flash-attn package (best performance, needs separate installation)\n• sdpa_flash: PyTorch SDPA Flash backend (excellent performance, best compatibility)\n• sage_attention: SageAttention wrapper (experimental, memory efficient)\n• sdpa_math: PyTorch SDPA math backend (stable fallback)\n• eager: Standard PyTorch attention (slowest, always works)\n• sdpa: Legacy option, auto-selects Flash or math",
  "preset_prompt": "Built-in instruction template describing how the model should analyze the media. Selecting '❌ None' sends no system instruction (use custom_prompt instead).",
  "custom_prompt": "When filled, completely replaces the preset prompt template. Leave empty to use the selected preset_prompt.",
  "max_tokens": "Maximum number of new tokens the model may generate. Larger values produce longer answers but take more time and memory. Both HF and GGUF backends respect this limit.",
  "keep_model_loaded": "Keep the model loaded in VRAM/RAM after inference so subsequent runs skip the loading step. Disable to free memory after each run.",
  "seed": "Random seed for sampling reproducibility and video frame selection. Reuse the same seed to reproduce identical results.",
  "use_torch_compile": "Enable torch.compile() optimization (HF models only, ignored for GGUF). Provides ~20-30% speedup after first compilation pass. Requires CUDA and PyTorch 2.1+.",
  "device": "Target device for inference. 'auto' selects the best available device. Use 'cpu' for CPU-only, 'cuda:0'/'cuda:1' for specific GPUs.",
  "temperature": "Controls sampling randomness. Lower values (0.1-0.3) produce focused, deterministic outputs; higher values (0.7+) produce more creative, varied outputs. Values below 0.01 trigger greedy decoding (no randomness). Used by both HF (when num_beams=1) and GGUF backends.",
  "top_p": "Nucleus sampling: only tokens whose cumulative probability exceeds this threshold are considered. Lower values (0.5) restrict to high-confidence tokens; higher values (0.9-0.95) allow more variety. Used by both HF (when num_beams=1) and GGUF backends.",
  "num_beams": "Beam search width (HF models only, ignored for GGUF). Values >1 disable temperature/top_p sampling and use beam search instead, which produces more stable but less creative outputs.",
  "repetition_penalty": "Penalizes repeated tokens. Values >1.0 (e.g. 1.1-1.3) reduce repetition; 1.0 applies no penalty. Used by both HF and GGUF backends.",
  "frame_count": "Number of frames uniformly sampled from video inputs. More frames provide richer temporal context but increase processing time and memory usage.",
  "min_pixels": "Minimum pixel count for image preprocessing (HF models only, ignored for GGUF). Controls the lower bound of image resolution. Default: 256×28×28 = 200,704.",
  "max_pixels": "Maximum pixel count for image preprocessing (HF models only, ignored for GGUF). Controls the upper bound of image resolution. Higher values give better detail but use more memory. Default: 1280×28×28 = 1,003,520.",
  "ctx": "Context window size in tokens (GGUF models only). Determines total prompt + response capacity. Higher values allow longer conversations but use more memory. Default: 8192.",
  "n_batch": "Prompt processing batch size (GGUF models only). Higher values can improve prompt ingestion throughput at the cost of memory. Default: 512.",
  "gpu_layers": "Number of model layers offloaded to GPU (GGUF models only). -1 offloads all layers (recommended for VRAM-sufficient GPUs). 0 runs entirely on CPU.",
  "image_max_tokens": "Maximum tokens allocated for image encoding (GGUF models only). Higher values preserve more image detail but consume more context window. Default: 4096.",
  "top_k": "Top-K parameter for llama.cpp model constructor (GGUF models only). Controls constructor-level top-K setting passed to Llama(). 0 = disabled. See also top_k_sampling for generation-time top-K.",
  "min_p": "Minimum probability sampling threshold (GGUF models only). Filters out tokens with probability below min_p × max_token_probability. Default 0.0 (disabled) matches HF behavior. Higher values (e.g. 0.05) aggressively narrow candidates, which may reduce output length.",
  "top_k_sampling": "Top-K sampling during generation (GGUF models only). Restricts each token choice to the K most probable candidates. Default 0 (disabled) matches HF behavior. Non-zero values (e.g. 40) reduce variety and may shorten output.",
  "pool_size": "Memory pool size for GGUF vision processing. Controls internal buffer allocation. Default: 4,194,304 (4 MB)."
}
