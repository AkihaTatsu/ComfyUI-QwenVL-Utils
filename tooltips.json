{
  "model_name": "Pick the Qwen-VL checkpoint. First run downloads weights into models/LLM/Qwen-VL, so leave disk space.",
  "model_type": "Choose between HuggingFace models (better quality) or GGUF models (lower VRAM usage).",
  "quantization": "Precision vs VRAM. FP16 gives the best quality if memory allows; 8-bit suits 8–16 GB GPUs; 4-bit fits 6 GB or lower but is slower.",
  "attention_mode": "auto tries Flash-Attn v2 -> SageAttention -> SDPA -> eager. Only override when debugging attention backends.",
  "preset_prompt": "Built-in instruction describing how Qwen-VL should analyze the media input.",
  "custom_prompt": "Optional override—when filled it completely replaces the preset template.",
  "max_tokens": "Maximum number of new tokens to decode. Larger values yield longer answers but consume more time and memory.",
  "keep_model_loaded": "Keeps the model resident in VRAM/RAM after the run so the next prompt skips loading.",
  "seed": "Seed controlling sampling and frame picking; reuse it to reproduce results.",
  "use_torch_compile": "Enable torch.compile('reduce-overhead') on supported CUDA/Torch 2.1+ builds for extra throughput after the first compile.",
  "device": "Choose where to run the model: auto, cpu, mps, or cuda:x for multi-GPU systems.",
  "temperature": "Sampling randomness when num_beams == 1. 0.2–0.4 is focused, 0.7+ is creative.",
  "top_p": "Nucleus sampling cutoff when num_beams == 1. Lower values keep only top tokens; 0.9–0.95 allows more variety.",
  "num_beams": "Beam-search width. Values >1 disable temperature/top_p and trade speed for more stable answers.",
  "repetition_penalty": "Values >1 (e.g., 1.1–1.3) penalize repeated phrases; 1.0 leaves logits untouched.",
  "frame_count": "Number of frames extracted from video inputs before prompting Qwen-VL. More frames provide context but cost time.",
  "min_pixels": "Minimum pixels for image processing. Lower values use less memory.",
  "max_pixels": "Maximum pixels for image processing. Higher values give better quality but use more memory.",
  "ctx": "Context length for GGUF models. Higher values allow longer conversations but use more memory.",
  "n_batch": "Batch size for GGUF inference. Higher values can improve throughput.",
  "gpu_layers": "Number of layers to offload to GPU for GGUF. -1 means all layers.",
  "image_max_tokens": "Maximum tokens for image encoding in GGUF models.",
  "top_k": "Top-K sampling parameter for GGUF models. 0 means disabled.",
  "pool_size": "Pool size for GGUF vision processing."
}
